# **Chapter 7. Predictive Modeling**

## Section 1. Drought Prediction

INSERT CONTENT FROM OVERLEAF

### Inspecting Drought Data

The data we will be using is the drought data, we will use it to train models that predict the $SPI$ and $SPEI$ indices.

```{r, echo=F, eval=T, message=F, warning=F}
## This chunk, code is hidden, only output is shown
# set directories
directory_path <- "C:/Users/KurtJi/OneDrive - University of Illinois - Urbana/Desktop/Geostatistics202306/data/drought_data/"
```

Read all data files in the directory and combine them to a single dataframe

```{r}
# load necessary libraries
library(readr)
library(tidyverse)
library(zoo)

file_names <- list.files(path = directory_path, pattern = "^[A-Z]+\\.csv$", full.names = TRUE)
data_list <- lapply(file_names, read_csv) # read each file and create a list of dataframes
drought_data <- reduce(data_list, full_join, by = "Time")
```

Process the data, including transforming the "Time" column into a Date object because we are dealing with a time-series data

```{r}
drought_data$Time <- as.Date(as.yearmon(drought_data$Time, "%Y %b")) # convert the Time column to a Date object
drought_data <- na.omit(drought_data) # remove rows with NAs

# plot SPI and SPEI against Time
ggplot(data = drought_data, aes(x = Time)) +
  geom_line(aes(y = SPI, colour = "SPI"), colour = "#1B5E20") +  # lighter Blue for SPI
  geom_line(aes(y = SPEI, colour = "SPEI"), colour = "#81D4FA") +  # darker Blue for SPEI
  labs(title = "Time Series of SPI and SPEI",
       x = "Time",
       y = "Index Value",
       colour = "Legend") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),  # rotate date labels for better readability
        legend.title = element_blank(),  # remove the legend title
        legend.position = "bottom",  # adjust legend position if necessary
        legend.text = element_text(size = 12))  # adjust text size for better legibility
```

Train/Test Split

```{r}
n <- nrow(drought_data)
train_size <- floor(0.7 * n)

train_indices <- 1:train_size
test_indices <- (train_size + 1):n

train_data <- drought_data[train_indices, ]
test_data <- drought_data[test_indices, ]

print(sprintf("Total records: %d", n))
print(sprintf("Training records: %d, Test records: %d", 
              nrow(train_data), nrow(test_data)))
```

#### Standardization of Data

Neural networks models are highly sensitive to the scale of input data. Those models typically use gradient-based optimization techniques to find the minimum of the loss function, and if the features have varying scales, it can cause disproportionate gradient updates. This can lead to slower convergence during training or even cause the training process to diverge. In addition, many neural networks use activation functions like sigmoid, tanh, or ReLU, which are sensitive to the magnitude of their inputs. Large input values can cause functions like sigmoid and tanh to saturate at their tails, where the gradient is near zero. This saturation can lead to vanishing gradients, where the gradient becomes too small for effective learning during backpropagation. Then, without standardization, features with larger scales can dominate the learning process by overshadowing the contributions of features with smaller scales. Standardizing ensures that each feature contributes equally to model learning.

An important note for standardizing data: Do not standardize the data altogether. Do them after the train/test split instead. This is because when you use information from outside the training dataset to train the model, you would have "data leakage". If the standardization parameters (mean and standard deviation) are derived from the entire dataset, then information from the test set has inadvertently been used to scale the training set. Data leakage is considered cheating in training models, and performance-wise, it can lead to overly optimistic performance estimates during training and poor generalization to new data.

```{r}

```

#### Lags in Time Series Analysis

## Section 2. Neural Networks

INSERT CONTENT FROM OVERLEAF

![Illustration of a neural network](neural_network.drawio.png)

**SHOULD WE INCLUDE CONCEPTS ABOUT EPOCHS, BATCH SIZE, DROPOUT, VALIDATION, ETC?**

#### Illustration of A Feed-Forward Neural Network

Keras in R is a high-level neural networks API that enables easy and fast prototyping of deep learning models. It operates on top of TensorFlow and is available in Python, too.

Building a neural network model in keras is straightforward as you just need to define all components using the `%>%` symbol. We are building a simple neural network with 2 linear layers, a dropout layer and a relu activation layer. We will let it take 3 variables for input: AMO, NAO and ONI

```{r}
library(keras)

feedforward_nn <- keras_model_sequential() %>%
  layer_dense(units = 64, input_shape = c(3), activation = 'relu') %>%  # first hidden layer with more units and ReLU activation
  layer_dense(units = 32, activation = 'relu') %>%  # additional hidden layer
  layer_dropout(rate = 0.5) %>%  # dropout layer to prevent overfitting
  layer_dense(units = 1, activation = 'linear')  # output layer with linear activation

summary(feedforward_nn) # show summary of the model
```

After we define and initialize the model, we need to compile

```{r}
feedforward_nn %>% compile(
  loss = 'mean_squared_error',
  optimizer = optimizer_rmsprop(),
  metrics = c('mean_absolute_error')
)
```

We will use NAO as the predictor to predict SPEI, the `fit()` operator will start the training process of the model. The number of training epochs is 100, and the batch size is 32. Batch size determins how many data samples the model sees before making a weight update. This parameter is to offer a balance between training speed and network stability. The validation split of 0.2 means that 20% of the training data is used for validation.

```{r, fold=TRUE}
train_x <- as.matrix(cbind(train_data$AMO, train_data$NAO, train_data$ONI))
train_y <- as.matrix(train_data$SPEI)

train_x <- scale(train_x)
train_y <- scale(train_y)

history <- feedforward_nn %>% fit(
  train_x,
  train_y,
  epochs = 100,
  batch_size = 32,
  validation_split = 0.2
)

```

Here we will fetch the training history and plot the change in training and validation loss

```{r}
plot(history)
```

We can see that the model has trained for 100 epochs. Now we can make predictions and check its accuracy on the testing data.

```{r}
test_x <- as.matrix(cbind(test_data$AMO, test_data$NAO, test_data$ONI))
test_y <- as.matrix(test_data$SPEI)

test_x <- scale(test_x)
test_y <- scale(test_y)

predictions <- predict(feedforward_nn, test_x)
# calculate the Mean Squared Error
mse <- mean((predictions - test_y) ^ 2)
print(mse)
```

In our basic model, we predict the SPEI index using the values of AMO, NAO, and ONI at each timestamp. However, this approach does not account for the time-dependency of the data, meaning it does not utilize information from previous timestamps. In the next section, we will explore more advanced neural networks that are specifically designed to handle sequential data by incorporating temporal dependencies.

### Deep LEARNING

INSERT CONTENT FROM OVERLEAF

### Recurrent Neural Networks

INSERT CONTENT FROM OVERLEAF

![Illustration of a recurrent neural network](RNN.drawio.png)

![Details of recurrent neural network](RNN_1.drawio.png)

### Long-Short-Term memory RNN and Drought prediction

INSERT CONTENT FROM OVERLEAF

![Details of LSTM network](LSTM_1.drawio.png)

#### An LSTM model that predicts 1 unit into the future

A simple example of a sequential model would be to use a set amount of past data to predict the nearest future data point. We will build an LSTM model that takes the SPEI values from 30 past data points and predicts the SPEI value for the next data point.

First, we need to organize our data into 'past data' and 'future data'. The function below iteratively partitions the SPEI index in the training data into many past-future pairs.

```{r}
# create a function to make the transformation
create_dataset <- function(data, steps=1) {
  X = list()
  Y = list()
  for (i in 1:(length(data) - steps)) {
    end_ix <- i + steps - 1
    X[[i]] <- data[i:end_ix]
    Y[[i]] <- data[end_ix + 1]
  }
  return(list(X = array(unlist(X), dim = c(length(X), steps, 1)), Y = array(unlist(Y), dim = c(length(Y)))))
}

# we are using 10 past data points to predict the next step
steps <- 30
train_transformed <- create_dataset(train_y, steps)
```

Now we can define the structure of our LSTM model. It begins with an LSTM layer featuring 50 units to capture temporal dependencies. Next, a Dense layer with a single unit predicts a future value. This configuration is basic yet robust enough to learn from sequences and make predictions based on historical data.

```{r}
# define the lstm model structure
lstm <- keras_model_sequential() %>%
  layer_lstm(units = 50, input_shape = c(steps, 1), return_sequences = FALSE) %>%  # lstm layer to process time series data
  layer_dense(units = 1)  # output layer to predict the next data point

# compile the model with mean squared error as the loss function and use an optimizer
lstm %>% compile(
  loss = 'mean_squared_error',
  optimizer = 'adam',
  metrics = c('mean_absolute_error')
)

# summary of the model to see the architecture and parameters
summary(lstm)
```

The LSTM model undergoes 100 training epochs with a batch size of 32. 20% of the training data is reserved for validation purposes.

```{r}
train_X <- train_transformed$X
train_Y <- train_transformed$Y

history <- lstm %>% fit(
  train_X,                          # training features
  train_Y,                          # target output
  epochs = 100,                     # number of epochs to train for
  batch_size = 32,                  # number of samples per gradient update
  validation_split = 0.2,           # percentage of data to use for validation
  shuffle = TRUE                    # shuffle training data before each epoch
)

plot(history)
```

### Additional Deep Learning Packages in R

#### H2O Package

H2O is a...

```{r}
library(h2o)
h2o.init()

h2o_data <- as.h2o(drought_data)

# you can do a train/test split in the h20 framework
train = h2o.splitFrame(h2o_data, ratios = c(0.7,0.15), seed =1)[[1]] 
valid = h2o.splitFrame(h2o_data, ratios = c(0.7,0.15), seed =1)[[2]] 
test = h2o.splitFrame(h2o_data, ratios = c(0.7,0.15), seed =1)[[3]] 
```

Specify model structure

```{r}
dl_model <- h2o.deeplearning(
  x = c("NAO", "NAO_lag1", "NAO_lag2"), # Using NAO and its lags as predictors
  y = "SPEI",                           # Target variable
  training_frame = train,
  validation_frame = valid,
  activation = "Rectifier",
  hidden = c(200, 100, 50),             # Two hidden layers with 50 neurons each
  epochs = 100
)
```

```{r}
# Make predictions on the test set
predictions <- h2o.predict(dl_model, newdata = test)
print(predictions)
```

```{r}
train
```

```{r}
actuals_vs_preds <- as.data.frame(test[,"SPEI"])
actuals_vs_preds$Predicted_SPEI <- as.vector(predictions)
```

```{r}
ggplot(actuals_vs_preds, aes(x = Time)) +
  geom_line(aes(y = SPEI, colour = "Actual"), linewidth = 1) +
  geom_line(aes(y = Predicted_SPEI, colour = "Predicted"), linewidth = 1) +
  labs(title = "Actual vs Predicted SPEI",
       x = "Time",
       y = "SPEI Index",
       colour = "Legend") +
  theme_minimal()
```

#### deepNet

deepNet is...

```{r}
library(deepnet)
```

**dbn is for weight initialization by a deep belief network**

```{r}
set.seed(123)  # For reproducibility
dbn_model <- dbn.dnn.train(train_x, train_y, hidden = c(25, 15), 
                       learningrate = 0.05, momentum = 0.1, 
                       output = "linear", numepochs = 100)
```

We can check the prediction accuracy from this model

```{r}
predictions <- nn.predict(dbn_model, test_x)

rmse <- sqrt(mean((predictions - test_y)^2))
print(paste("Root Mean Squared Error (RMSE):", rmse))
```

#### neuralNet

neuralNet is...

You can use `~` to specify the formula, but here we need to re-define the data so that it has both the predictors and the truth

```{r}
library(neuralnet)

set.seed(123)
nn <- neuralnet(SPEI ~ AMO + NAP + ONI, 
                data = train_data, 
                hidden = c(4,2), 
                linear.output = TRUE,
                threshold = 0.01)
```

```{r}
plot(nn,rep = "best")
```

```{r}
pred <- predict(model, test_data)

# Actual vs Predicted
predicted_values <- nn_results$net.result
actual_values <- test_data$SPEI

# Calculate RMSE
rmse <- sqrt(mean((predicted_values - actual_values)^2))
print(paste("Root Mean Squared Error (RMSE):", rmse))

# Visual comparison
plot(actual_values, type = 'o', col = 'blue', ylim = range(c(actual_values, predicted_values)), pch = 16, xlab = "Index", ylab = "SPEI")
points(predicted_values, type = 'o', pch = 15, col = 'red')
legend("bottomright", legend = c("Actual", "Predicted"), col = c("blue", "red"), pch = c(16, 15))
```

## Section 3. Support Vector Machines

## Section 4. Multivariate Adaptive Regression Splines

## Section 5. Regression Trees and Random Forests
